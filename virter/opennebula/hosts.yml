---
all:
  vars:
    ansible_user: root
    one_version: "6.10"
    one_pass: opennebulapass
    features:
      # Enable the "ceph" feature in one-deploy.
      ceph: true
    ds:
      # Simple datastore setup - use built-in Ceph cluster for datastores 0 (system) and 1 (images).
      mode: ceph
    vn:
      admin_net:
        managed: true
        template:
          VN_MAD: bridge
          PHYDEV: eth0
          BRIDGE: br0
          AR:
            TYPE: IP4
            IP: 172.20.0.100
            SIZE: 48
          NETWORK_ADDRESS: 192.168.124.0
          NETWORK_MASK: 255.255.255.0
          GATEWAY: 192.168.124.1
          DNS: 1.1.1.1

frontend:
  hosts:
    f1: { ansible_host: opennebula-cluster-11 }

node:
  hosts:
    n1: { ansible_host: opennebula-cluster-12 }
    n2: { ansible_host: opennebula-cluster-13 }
    n3: { ansible_host: opennebula-cluster-14 }

grafana:
  hosts:
    f1: { ansible_host: opennebula-cluster-11 }

ceph:
  children:
    mons:
    mgrs:
    osds:
  vars:
    # osd_memory_target: 4294967296 # 4GiB (default)
    osd_memory_target: 1294967296 # 1GiB
    # Assuming all osds are of equal size, setup resource limits and reservations
    # for all osd systemd services.
    ceph_osd_systemd_overrides:
      Service:
        CPUWeight: 200 # 100 is the kernel default
        CPUQuota: 100% # 1 full core
        MemoryMin: "{{ (0.75 * osd_memory_target) | int }}"
        MemoryHigh: "{{ osd_memory_target | int }}"
    # Make sure osds preserve memory if it's below the value of the "osd_memory_target" fact.
    ceph_conf_overrides:
      osd:
        osd memory target: "{{ osd_memory_target | int }}"
    osd_auto_discovery: true

mons:
  hosts:
    f1:
      {
        ansible_host: opennebula-cluster-11,
        monitor_address: opennebula-cluster-11,
      }

mgrs:
  hosts:
    f1: { ansible_host: opennebula-cluster-11 }

osds:
  hosts:
    n1: { ansible_host: opennebula-cluster-12 }
    n2: { ansible_host: opennebula-cluster-13 }
    n3: { ansible_host: opennebula-cluster-14 }
